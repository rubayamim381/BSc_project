{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Minhash comparison time.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPdo0B/46PRRFJxnq4GXEl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"A82Z8a52E2AF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644004947538,"user_tz":-360,"elapsed":35007,"user":{"displayName":"Rubaya Akter Mim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghv7Q5aMTaQ6Dp7vI4cEieVFqcP2X_tI4xMNkEnKQ=s64","userId":"14574301021038508617"}},"outputId":"17b629fa-c6b7-4e30-a204-afc376ed5b61"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gsu93XfCyOu","executionInfo":{"status":"ok","timestamp":1644005249445,"user_tz":-360,"elapsed":1573,"user":{"displayName":"Rubaya Akter Mim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghv7Q5aMTaQ6Dp7vI4cEieVFqcP2X_tI4xMNkEnKQ=s64","userId":"14574301021038508617"}},"outputId":"b16d92e5-b609-45f4-cd69-0058048eb2e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\t...Shingling articles... \n","\n","Shingling 100 docs took 0.03 sec.\n","\n","Average shingles per doc: 243.69\n","\n","\n","\t...Generating MinHash signatures for all documents...\n","\n","Generating MinHash signatures for  100  number of permutations took 1.32sec\n","\n","\n","\t...Comparing all signatures...\n","\n","List of Document Pairs with J(d1,d2) more than  0.8 is considered as a similarity\n","\n","╒═════════╤═════════╤═══════════════╤════════════╤═════════════╕\n","│ Doc_1   │ Doc_2   │   Estimated J │   Actual J │    Distance │\n","╞═════════╪═════════╪═══════════════╪════════════╪═════════════╡\n","│ t980    │ t2023   │          0.97 │   0.979167 │ 0.00916667  │\n","├─────────┼─────────┼───────────────┼────────────┼─────────────┤\n","│ t9536   │ t9537   │          0.99 │   0.975    │ 0.015       │\n","├─────────┼─────────┼───────────────┼────────────┼─────────────┤\n","│ t1088   │ t5015   │          0.99 │   0.980545 │ 0.00945525  │\n","├─────────┼─────────┼───────────────┼────────────┼─────────────┤\n","│ t1297   │ t4638   │          0.98 │   0.98062  │ 0.000620155 │\n","├─────────┼─────────┼───────────────┼────────────┼─────────────┤\n","│ t1768   │ t5248   │          0.99 │   0.980315 │ 0.00968504  │\n","├─────────┼─────────┼───────────────┼────────────┼─────────────┤\n","│ t1952   │ t3495   │          0.99 │   0.978448 │ 0.0115517   │\n","╘═════════╧═════════╧═══════════════╧════════════╧═════════════╛\n","\n","Whole process took 1.46 seconds\n","\n","\n","\tFor  100  number of permutations, MinHash technique took 0.14 seconds for comparing all signatures\n"]}],"source":["from __future__ import division\n","import os\n","import re\n","import sys\n","import random\n","import time\n","import binascii\n","from bisect import bisect_right\n","from heapq import heappop, heappush\n","import pandas as pd\n","import numpy as np\n","from tabulate import tabulate\n","from itertools import combinations\n","#from beautifultable import BeautifulTable\n","\n","# This is the number of components in the resulting MinHash signatures.\n","numHashes = 100\n","\n","\n","# It ships with data set sizes 100, 1000, 2500, and 10000.\n","numDocs = 100\n","trainFile = \"/content/drive/MyDrive/Colab Notebooks/data/articles_\" + str(numDocs) + \".train.txt\"\n","testFile = \"/content/drive/MyDrive/Colab Notebooks/data/articles_\" + str(numDocs) + \".test.txt\"\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/data/file3.txt', 'w', encoding=\"utf-8\") as dataFile:\n","  \n","    # Iterate through list\n","    for names in [trainFile, testFile]:\n","  \n","        # Open each file in read mode\n","        with open(names, encoding=\"utf-8\") as infile:\n","  \n","            # read the data from file1 and file2 and write it in file3\n","            dataFile.write(infile.read())\n","        dataFile.write(\"\\n\")\n","\n","\n","#truthFile = \"./data/articles_\" + str(numDocs) + \".truth.txt\"\n","\n","\n","# =============================================================================\n","#                     LSH techniques started from here\n","# =============================================================================\n","\n","\n","#Convert Documents To Sets of Shingles\n","\n","print (\"\\n\\t...Shingling articles... \")\n","\n","# The current shingle ID value to assign to the next new shingle we encounter.\n","curShingleID = 0\n","\n","# Create a dictionary of the articles, mapping the article identifier (e.g.,\"t8470\") to the list \n","#of shingle IDs that appear in the document.\n","docsAsShingleSets = {};\n","  \n","\n","f = open('/content/drive/MyDrive/Colab Notebooks/data/file3.txt', \"r\" , encoding=\"utf8\")\n","\n","docNames = []\n","\n","t0 = time.time()\n","\n","totalShingles = 0\n","\n","docNM = []\n","\n","for i in range(0, numDocs):\n","  \n","  words = f.readline().split(\" \")  \n","  \n","  docID = words[0]\n","  \n","  # Maintain a list of all document IDs.  \n","  docNames.append(docID)\n","    \n","  del words[0]  \n","  \n","  shinglesInDoc = set()\n","  \n","  # For each word in the document...\n","  for index in range(0, len(words) - 2):\n","\n","    #shingle = list(words[index].union(words[index + 1]).union(words[index + 2]))\n","    shingle = words[index] + \" \" + words[index + 1] + \" \" + words[index + 2]\n","    #print(shingle)\n","    #print(\"\\n\")\n","    \n","\n","    # Hash the shingle to a 32-bit integer.\n","    crc = binascii.crc32(shingle.encode('utf8')) & 0xffffffff\n","    \n","    # Add the hash value to the list of shingles for the current document.  \n","    shinglesInDoc.add(crc)\n","  \n","  docsAsShingleSets[docID] = shinglesInDoc\n","  #print (shinglesInDoc)\n","  #print(\"\\n\")\n","  \n","  # Count the number of shingles across all documents.\n","  totalShingles = totalShingles + (len(words) - 2)\n","\n","f.close()  \n","\n","\n","\n","# Report how long shingling took.\n","print (\"\\nShingling \" + str(numDocs) + \" docs took %.2f sec.\" % (time.time() - t0))\n"," \n","print ('\\nAverage shingles per doc: %.2f' % (totalShingles / numDocs))\n","\n","# =============================================================================\n","#                     Define Triangle Matrices\n","# =============================================================================\n","\n","# Calculate the number of elements needed in our triangle matrix\n","numElems = int(numDocs * (numDocs - 1) / 2)\n","\n","# Initialize two empty lists to store the similarity values\n","#for the actual Jaccard Similarity values\n","JSim = [0 for x in range(numElems)]\n","#for the estimated Jaccard Similarities found by comparing the MinHash signatures.\n","estJSim = [0 for x in range(numElems)]\n","\n","\n","\n","# Define a function to map a 2D matrix coordinate into a 1D index.\n","def getTriangleIndex(i, j):\n","  # If i == j that's an error.\n","  if i == j:\n","    sys.stderr.write(\"Can't access triangle matrix with i == j\")\n","    sys.exit(1)\n","  # If j < i just swap the values.\n","  if j < i:\n","    temp = i\n","    i = j\n","    j = temp\n","  \n","  # Calculate the index within the triangular array.\n","  # This fancy indexing scheme is taken from pg. 211 of:\n","  # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n","  # But I adapted it for a 0-based index.\n","  # Note: The division by two should not truncate, it\n","  #       needs to be a float. \n","  k = int(i * (numDocs - (i + 1) / 2.0) + j - i) - 1\n","  \n","  return k\n","\n","        \n","# =============================================================================\n","#                 Generate MinHash Signatures\n","# =============================================================================\n","\n","# Time this step.\n","#t0 = time.time()\n","\n","#print (\"\\nGenerating random hash functions...\")\n","\n","# Record the maximum shingle ID that we assigned.\n","maxShingleID = 2**32-1\n","\n","# We need the next largest prime number above 'maxShingleID'.\n","# I looked this value up here: \n","# http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n","nextPrime = 4294967311\n","\n","\n","# Our random hash function will take the form of:\n","#   h(x) = (a*x + b) % c\n","# Where 'x' is the input value, \n","#'a' and 'b' are random coefficients, \n","#and 'c' is a prime number just greater than maxShingleID.\n","\n","# Generate a list of 'k' random coefficients for the random hash functions\n","def pickRandomCoeffs(k):\n","  # Create a list of 'k' random values.\n","  randList = []\n","  \n","  while k > 0:\n","    # Get a random shingle ID.\n","    randIndex = random.randint(0, maxShingleID) \n","  \n","    # Ensure that each random number is unique.\n","    while randIndex in randList:\n","      randIndex = random.randint(0, maxShingleID) \n","    \n","    # Add the random number to the list.\n","    randList.append(randIndex)\n","    k = k - 1\n","    \n","  return randList\n","\n","# For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n","coeffA = pickRandomCoeffs(numHashes)\n","coeffB = pickRandomCoeffs(numHashes)\n","\n","print (\"\\n\\n\\t...Generating MinHash signatures for all documents...\")\n","\n","# List of documents represented as signature vectors\n","signatures = []\n","\n","# For each document...\n","for docID in docNames:\n"," \n","  shingleIDSet = docsAsShingleSets[docID]\n","\n","  signature = []\n","  \n","  # For each of the random hash functions...\n","  for i in range(0, numHashes):\n","    \n","    # For each of the shingles actually in the document, calculate its hash code using hash function 'i'. \n","    minHashCode = nextPrime + 1\n","    \n","    # For each shingle in the document...\n","    for shingleID in shingleIDSet:\n","      # Evaluate the hash function.\n","      hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n","      \n","      # Track the lowest hash code seen.\n","      if hashCode < minHashCode:\n","        minHashCode = hashCode\n","\n","    # Add the smallest hash code value as component number 'i' of the signature.\n","    signature.append(minHashCode)\n","    # Calculate the elapsed time (in seconds)\n","  #print(signature)\n","  \n","  #Store the MinHash signature for this document.\n","  signatures.append(signature)\n","\n","#print(signatures[:5])\n","#print(\"\\n\")\n","# Calculate the elapsed time (in seconds)\n","elapsed1 = (time.time() - t0)\n","print (\"\\nGenerating MinHash signatures for \", numHashes, \" number of permutations took %.2fsec\" % elapsed1)  \n","\n","# =============================================================================\n","#                     Compare All Signatures\n","# =============================================================================  \n","\n","print (\"\\n\\n\\t...Comparing all signatures...\")  \n","  \n","# N x N matrix\n","#t0 = time.time()\n","\n","# For each of the test documents...\n","for i in range(0, numDocs):\n","  # Get the MinHash signature for document i.\n","  signature1 = signatures[i]\n","    \n","  # For each of the other test documents...\n","  for j in range(i + 1, numDocs):\n","    \n","    # Get the MinHash signature for document j.\n","    signature2 = signatures[j]\n","    \n","    count = 0\n","    # Count the number of positions in the minhash signature which are equal.\n","    for k in range(0, numHashes):\n","      count = count + (signature1[k] == signature2[k])\n","    \n","    # Record the percentage of positions which matched.    \n","    estJSim[getTriangleIndex(i, j)] = (count / numHashes)\n","\n","\n","    \n","    \n","# =============================================================================\n","#                   Display Similar Document Pairs\n","# =============================================================================  \n","\n","# Count the true positives and false positives.\n","data = []\n","tp = 0\n","fp = 0\n","\n","col_names = [\"Doc_1\", \"Doc_2\",\"Estimated J\",\"Actual J\",\"Distance\"]\n","  \n","threshold = 0.8  \n","print (\"\\nList of Document Pairs with J(d1,d2) more than \", threshold, \"is considered as a similarity\\n\")\n","\n","#print (\"                   Est. J   Act. J\")\n","\n","\n","# For each of the document pairs...\n","for i in range(0, numDocs):  \n","  for j in range(i + 1, numDocs):\n","    # Retrieve the estimated similarity value for this pair.\n","    estJ = estJSim[getTriangleIndex(i, j)]\n","    \n","    \n","    if estJ > threshold:\n","    \n","      # Calculate the actual Jaccard similarity for validation.\n","      s1 = docsAsShingleSets[docNames[i]]\n","      s2 = docsAsShingleSets[docNames[j]]\n","      J = (len(s1.intersection(s2)) / len(s1.union(s2)))\n","      distance = abs(float(estJ - J))\n","      \n","      data += [[docNames[i], docNames[j], estJ,  J, distance]]\n","\n","print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))\n","\n","# Calculate the elapsed time (in seconds)\n","elapsed2 = (time.time() - t0)\n","print(\"\\nWhole process took %.2f seconds\" % elapsed2)\n","print (\"\\n\\n\\tFor \", numHashes, \" number of permutations, MinHash technique took %.2f seconds\" % (elapsed2 - elapsed1), \"for comparing all signatures\")\n","\n","#table.append_row([docNames[i],  docNames[j],  estJ,  J])\n","#print(table)\n","#display table\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"e6IvOvKgDUfu"},"execution_count":null,"outputs":[]}]}